<!-- VIM :set ts=2 sts=2 sw=2 et -->
<!-- Copy this file, and edit the copy. -->
<cluster>
  <environment
env.name="icehouse_ha"
node.username="root"
node.password=""
node.ssh_key="/root/.ssh/id_rsa"

pacemaker.cluster.endpoints='["zy0", "zy1", "zy2"]'
pacemaker.cluster.haproxy='["zy4", "zy5", "zy6"]'

openstack.ha.resource_manager="pacemaker"

openstack.mq.service_type="qpid"
openstack.db.service_type="mysql"
openstack.db.db_name="openstac"
openstack.db.vip="10.0.1.120"

openstack.db.compute.db_name="nova"
openstack.db.compute.username="nova1"
openstack.db.identity.db_name="keyston"
openstack.db.identity.username="keyston1"
openstack.db.image.db_name="glance"
openstack.db.image.username="glance1"
openstack.db.network.db_name="neutron"
openstack.db.network.username="neutron1"
openstack.db.block-storage.db_name="cinder"
openstack.db.block-storage.username="cinder1"
openstack.db.dashboard.db_name="horizon"
openstack.db.dashboard.username="horizon1"
openstack.db.telemetry.db_name="ceilome"
openstack.db.telemetry.username="ceilome1"
openstack.db.orchestration.db_name="heat"
openstack.db.orchestration.username="heat1"

openstack.endpoints.db.nodes='["10.0.1.151", "10.0.1.152", "10.0.1.150"]'
openstack.endpoints.db.port="3306"
openstack.endpoints.db.ha_enabled="true"

openstack.mq.qpid.vip="10.0.1.121"
openstack.mq.qpid.nodes='["10.0.1.151", "10.0.1.152", "10.0.1.150"]'
openstack.endpoints.mq.port="5672"

openstack.ha.endpoints.vip="10.0.1.120"
openstack.ha.nodes='["10.0.1.154", "10.0.1.155", "10.0.1.156"]'

openstack.endpoints.identity-api.nodes='["10.0.1.151", "10.0.1.152", "10.0.1.150"]'
openstack.endpoints.compute-api.nodes='["10.0.1.151", "10.0.1.152", "10.0.1.150"]'
openstack.endpoints.compute-api.host="10.0.1.151"
openstack.endpoints.network-api.nodes='["10.0.1.151", "10.0.1.152", "10.0.1.150"]'
openstack.endpoints.image-api.nodes='["10.0.1.151", "10.0.1.152", "10.0.1.150"]'
openstack.endpoints.block-storage-api.nodes='["10.0.1.151", "10.0.1.152", "10.0.1.150"]'
openstack.endpoints.telemtry-api.nodes='["10.0.1.151", "10.0.1.152", "10.0.1.150"]'
openstack.endpoints.orchestration-api.nodes='["10.0.1.151", "10.0.1.152", "10.0.1.150"]'
openstack.endpoints.orchestration-api-cfn.nodes='["10.0.1.151", "10.0.1.152", "10.0.1.150"]'
openstack.endpoints.orchestration-api-cloudwatch.nodes='["10.0.1.151", "10.0.1.152", "10.0.1.150"]'

openstack.network.openvswitch.local_ip_interface="eth1"
openstack.network.l3.external_network_bridge_interface="eth0"

openstack.image.upload_images='["cirros", "precise"]'
openstack.image.upload_image.precise="http://10.0.1.39:8081/static/images/precise-server-cloudimg-amd64-disk1.img"
openstack.image.upload_image.cirros="http://10.0.1.39:8081/static/images/cirros-0.3.0-x86_64-uec.tar.gz"

yum.repo.rhel="ftp://yuezhy%40cn.ibm.com:zhangyue123@ftp3-ca.linux.ibm.com/redhat/abat_yum/rhel-6-u5/"
yum.repo.openstack.noarch="http://rchgsa.ibm.com/projects/e/emsol/ccs/build/driver/icehouse-proposed/openstack/D20140623-0132-2014-1-1-ibm-rc2-KEEP/noarch/"
yum.repo.openstack.x86_64="http://rchgsa.ibm.com/projects/e/emsol/ccs/build/driver/icehouse-proposed/openstack/D20140623-0132-2014-1-1-ibm-rc2-KEEP/x86_64/"
/>

  <nodes>
    <node>
      <host>zy1</host>
      <roles>pacemaker,mysql-ha,pacemaker-mysql-config,openstack-db,qpid-ha,pacemaker-qpid-config,os-identity,os-image,os-network,os-compute-setup,os-compute-api,os-compute-scheduler,os-compute-conductor,os-compute-vncproxy,os-compute-cert,os-compute-setup,os-block-storage,os-orchestration,pacemaker-endpoints-deploy,pacemaker-endpoints-config</roles>
    </node>
    <node>
      <host>zy2</host>
      <roles>pacemaker,mysql-ha,qpid-ha,os-identity,os-image,os-network,os-compute-setup,os-compute-api,os-compute-scheduler,os-compute-conductor,os-compute-vncproxy,os-compute-cert,os-compute-setup,os-block-storage,os-orchestration,pacemaker-endpoints-deploy</roles>
    </node>
    <node>
      <host>zy0</host>
      <roles>pacemaker,mysql-ha,qpid-ha,os-identity,os-image,os-network,os-compute-setup,os-compute-api,os-compute-scheduler,os-compute-conductor,os-compute-vncproxy,os-compute-cert,os-compute-setup,os-block-storage,os-orchestration,pacemaker-endpoints-deploy</roles>
    </node>
    <node>
      <host>zy3</host>
        <roles>os-compute-worker,os-network-openvswitch</roles>
    </node>
    <node>
      <host>zy4</host>
        <roles>pacemaker,haproxy-ha,pacemaker-haproxy-config</roles>
    </node>
    <node>
      <host>zy5</host>
        <roles>pacemaker,haproxy-ha</roles>
    </node>
    <node>
      <host>zy6</host>
        <roles>pacemaker,haproxy-ha</roles>
    </node>
  </nodes>

  <order sequence="set1,set2,set3,set4,set5,set6,set7,set8,set9">
    <set1>pacemaker</set1>
    <set2>mysql-ha,qpid-ha,haproxy-ha</set2>
    <set3>pacemaker-mysql-config,pacemaker-qpid-config,pacemaker-haproxy-config</set3>
    <set4>openstack-db</set4>
    <set5>os-identity</set5>
    <set6>os-image,os-network,os-block-storage,os-compute-setup</set6>
    <set7>os-compute-api,os-compute-scheduler,os-compute-conductor,os-compute-vncproxy,os-compute-cert,os-orchestration</set7>
    <set8>os-network-openvswitch,pacemaker-endpoints-deploy</set8>
    <set9>os-compute-worker,os-dashboard,pacemaker-endpoints-config</set9>
  </order>
</cluster>
